Spectrogram-based pattern recognition algorithms, such as sparse coding [Abdallah & Plumbley 2005] and Nonnegative Matrix Factorization [Smaragdis & Brown 2003], in audio signal processing. They are designed to approximate their input by a linear combination of few data-driven templates. Musical chords, for instance, are expected to get decomposed into individual notes. However, most real-world sounds cannot factorized as amplitude-modulated fixed spectra : notably, continuous changes in pitch (vibrato and glissando) as well as in spectral envelope (attack transients and formantic transitions) have a joint time-frequency structure that cannot be matched to a single spectral atom. Time-varying generalizations have been devised to address this shortcoming [Hennequin et al. 2011], but their high number of parameters prevents their robustness in challenging polyphonic contexts.
Instead of specifying probabilistic priors to help the convergence [Fuentes et al. 2013], we aim to design a template-free, nonlinear, mid-level representation, that disentangles the time variability of pitch and spectral envelope.
It suggests that basic implementations of existing algorithms could be 